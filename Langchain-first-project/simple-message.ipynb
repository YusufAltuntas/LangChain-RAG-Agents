{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from fastapi import FastAPI\n",
    "from langserve import add_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='En sevdiğim kitap türü fantastik kurgu.', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f33667828e', 'finish_reason': 'stop', 'logprobs': None}, id='run-1da54395-0702-4822-bcc8-a7418db9b31b-0', usage_metadata={'input_tokens': 27, 'output_tokens': 12, 'total_tokens': 39})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to Turkish.\"),\n",
    "    HumanMessage(content=\"My best book genre is fantastic fiction.\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output Parser Nedir?**\n",
    "\n",
    "**Output parser**, bir modelden elde edilen ham çıktıların anlamlı ve kullanılabilir bir forma dönüştürülmesini sağlayan bir bileşendir. Genellikle, dil modelleri (örneğin, metin üretimi yapan modeller) belirli bir formatta veri üretir, ancak bu çıktıyı uygulamanızın gereksinimlerine uygun hale getirmek için ek bir işleme ihtiyaç duyabilirsiniz.\n",
    "\n",
    "### **LangChain'de Output Parser'ın Rolü**\n",
    "\n",
    "LangChain'de output parser, genellikle şu görevleri üstlenir:\n",
    "\n",
    "1. **Çıktıyı Formatlama:** Modelin ürettiği metni, belirli bir yapı veya formatta düzenler. Örneğin, modelden alınan serbest biçimli metni JSON veya CSV gibi yapılandırılmış bir formata dönüştürebilir.\n",
    "\n",
    "2. **Veri Çıkarma:** Modelin yanıtından belirli bilgileri çıkarmak için kullanılır. Örneğin, bir metin içinde tarihleri, isimleri veya diğer önemli verileri ayıklamak.\n",
    "\n",
    "3. **Hata Yönetimi:** Model çıktısındaki hataları veya anormallikleri tespit edip düzeltebilir. Bu, modelin ürettiği verinin doğruluğunu artırır.\n",
    "\n",
    "4. **Veri Doğrulama:** Çıktının belirli bir şemaya veya kurallara uygun olup olmadığını kontrol eder.\n",
    "\n",
    "\n",
    "### **Özet**\n",
    "\n",
    "Output parser, LangChain'de model çıktılarının anlamlı ve kullanılabilir bir biçimde işlenmesini sağlayan önemli bir bileşendir. Bu araçlar, model sonuçlarının uygulama gereksinimlerine uygun hale getirilmesine yardımcı olur ve veri işleme sürecini daha verimli hale getirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En sevdiğim kitap türü fantastik kurgu.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to Turkish.\"),\n",
    "    HumanMessage(content=\"My best book genre is fantastic fiction.\")\n",
    "]\n",
    "\n",
    "parser = StrOutputParser()\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En sevdiğim kitap türü fantastik kurgu.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to Turkish.\"),\n",
    "    HumanMessage(content=\"My best book genre is fantastic fiction.\")\n",
    "]\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = llm | parser\n",
    "\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain'de **`ChatPromptTemplate`** sınıfı, bir dil modeline (LLM) gönderilecek isteklerin biçimlendirilmesini ve yapılandırılmasını sağlar. `ChatPromptTemplate`, dil modelinin belirli bir formatta yanıt vermesini ve verilen şablonlara göre çalışmasını kolaylaştırır.\n",
    "\n",
    "### **`ChatPromptTemplate` Nedir?**\n",
    "\n",
    "`ChatPromptTemplate`, dil modeline bir veya birden fazla mesaj türünde bilgi sağlamanın bir yolunu sunar. Bu mesajlar genellikle iki bölümden oluşur:\n",
    "\n",
    "1. **Sistem Mesajları (System Messages):** Modelin genel talimatlarını veya görev tanımlarını içerir. Bu mesajlar, modelin görevini nasıl yerine getirmesi gerektiğini belirler.\n",
    "\n",
    "2. **Kullanıcı Mesajları (User Messages):** Modelin yanıtlaması gereken spesifik bilgi veya verileri içerir. Kullanıcıdan alınan bu veriler, modelin belirli bir çıktıyı üretmesini sağlar.\n",
    "\n",
    "### **`ChatPromptTemplate` Kullanımı**\n",
    "\n",
    "**1. Şablon Oluşturma:**\n",
    "\n",
    "`ChatPromptTemplate`, mesajları şablon şeklinde yapılandırmanıza olanak tanır. Şablonlar genellikle metin içinde yer tutucular kullanır. Örneğin, `{language}` veya `{text}` gibi yer tutucular, dinamik olarak değiştirilebilecek yerleri temsil eder.\n",
    "\n",
    "**2. Şablonun Kullanılması:**\n",
    "\n",
    "Şablonlar, dil modeline belirli bir formatta bilgi sağlamak için kullanılır. Model bu şablona göre yanıt üretir. Şablon, modelin istenen formatta ve içeriğe uygun şekilde yanıt vermesine yardımcı olur.\n",
    "\n",
    "**Örnekle Açıklama:**\n",
    "\n",
    "Aşağıda verilen kod örneğinde olduğu gibi, şablonlar şu şekilde çalışır:\n",
    "\n",
    "1. **Sistem Mesajı:** Modelin görevini tanımlar. Örneğin, \"Translate following into {language}\" mesajı, modelden verilen metni belirli bir dile çevirmesi istenir.\n",
    "\n",
    "2. **Kullanıcı Mesajı:** Modelin işlem yapacağı asıl veriyi içerir. Örneğin, `{text}` yer tutucusu, çevrilecek metni temsil eder ve bu metin şablona yerleştirilir.\n",
    "\n",
    "**Temel Adımlar:**\n",
    "\n",
    "1. **Şablonun Tanımlanması:** Şablon, modelin yanıt vermesi için gereken genel talimatları ve spesifik verileri içerir. Şablondaki yer tutucular, dinamik verilerle doldurulacaktır.\n",
    "\n",
    "2. **Şablonun Uygulanması:** Şablon oluşturulduktan sonra, gerçek verilerle doldurulur ve dil modeline gönderilir. Model, şablona göre formatlanmış veriyi alır ve yanıt üretir.\n",
    "\n",
    "### **Özet**\n",
    "\n",
    "`ChatPromptTemplate`, dil modellerine verilecek isteklerin ve yanıtların yapılandırılmasını sağlayan bir şablondur. Bu şablonlar, modelin belirli bir formatta ve içeriğe uygun yanıtlar üretmesini sağlar. Şablonlar, genellikle sistem ve kullanıcı mesajları olarak ikiye ayrılır ve dinamik yer tutucular kullanarak çeşitli veri türlerini işleyebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao Mondo!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "system_prompt = \"Translate following into {language}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | parser # prompt_template i al modele ver, çıktıyı al parser a ver\n",
    "\n",
    "chain.invoke({\"language\": \"Italian\", \"text\": \"Hello World!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LangServe Kullanımı**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [37788]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/chain/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /chain/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\n",
      "INFO:     ::1:56973 - \"GET /chain HTTP/1.1\" 404 Not Found\n",
      "INFO:     ::1:56973 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     ::1:57004 - \"GET /chain/playground HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     ::1:57004 - \"GET /chain/playground/ HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57004 - \"GET /chain/playground/assets/index-dbc96538.js HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57005 - \"GET /chain/playground/assets/index-52e8ab2f.css HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57005 - \"GET /chain/playground/favicon.ico HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57024 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57071 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57075 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57258 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57263 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57270 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57276 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "system_prompt = \"Translate following into {language}\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | llm | parser # prompt_template i al modele ver, çıktıyı al parser a ver\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Translator App\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"Translation Chat Bot\"\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\"\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
